{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Data Cleansing\n",
        "\n",
        "\n",
        "*   **Dropped rows** where key columns (`MMSI`, `Latitude`, `Longitude`, `timestamp`) are null.\n",
        "*   **Removed duplicates** based on `MMSI`, `Latitude`, `Longitude`, and `timestamp`.\n",
        "* **Filtered out** invalid latitude and longitude values (Latitude: -90 to 90, Longitude: -180 to 180).\n",
        "* **Excluded rows** with unrealistic speeds (greater than 107 km/h) based on calculated speed. Treshold was selected based on current quickest ship - HSC Francisco.  \n",
        "\n",
        "\n",
        "Longest distance Travelled\n",
        "*  Using `geopy` module, we calculated the distance traveled between consecutive points for each vessel. This step required handling missing values carefully to avoid calculation errors.\n",
        "4. We identified the vessel as `219133000` with `790 km`"
      ],
      "metadata": {
        "id": "rK-KbrKKvsXz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data retrieval"
      ],
      "metadata": {
        "id": "sZrO7kOuwxXZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGrPE6uDvleG"
      },
      "outputs": [],
      "source": [
        "# Download the zipped dataset\n",
        "!wget http://web.ais.dk/aisdata/aisdk-2024-05-04.zip -O aisdk-2024-05-04.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "zip_file_path = 'aisdk-2024-05-04.zip'\n",
        "extract_to = 'ais_data'\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)\n",
        "\n",
        "extracted_files = os.listdir(extract_to)\n",
        "print(f\"Extracted files: {extracted_files}\")"
      ],
      "metadata": {
        "id": "ll_rCA7aw12x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hI6YIdiLw5wp",
        "outputId": "bac502e2-7e28-4c33-816f-4f501deab8eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_file_path = os.path.join(extract_to, extracted_files[0])  # Assuming there's only one file in the ZIP\n",
        "destination_dir = \"dest/dir\"\n",
        "\n",
        "os.makedirs(destination_dir, exist_ok=True)\n",
        "\n",
        "destination_file_path = os.path.join(destination_dir, extracted_files[0])\n",
        "shutil.copy(source_file_path, destination_file_path)\n",
        "\n",
        "print(f\"File copied to: {destination_file_path}\")"
      ],
      "metadata": {
        "id": "yZrYhwWmxEI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark in Google Colab"
      ],
      "metadata": {
        "id": "ujMnZzt7xJ9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"Our First Spark Example\") \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        },
        "id": "XUcKFKvUxOhO",
        "outputId": "92c55b11-efba-40cd-d4ce-6e1920ab6e66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com] [1 In\u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com] [Conn\u001b[0m\r                                                                               \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.91.83)] [Connecting to security.ub\u001b[0m\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Connecting to ppa.launchpadcontent.net (185.125.190.8\u001b[0m\r                                                                               \rGet:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:8 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,083 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,125 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,375 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.0 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [110 kB]\n",
            "Fetched 5,070 kB in 1s (3,852 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "45 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=8b72068ec05adfaed5aa31e8c86e94ef7be47377a8c9ab0dd67c4c742175ea72\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7d7291f471c0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://468c91e6c447:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Our First Spark Example</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Script"
      ],
      "metadata": {
        "id": "rcn1IppzxQIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install geopy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujMCu46q244_",
        "outputId": "cd707b27-f3df-413f-c91c-e15b1a6758bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: geopy in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
            "Requirement already satisfied: geographiclib<3,>=1.52 in /usr/local/lib/python3.10/dist-packages (from geopy) (2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, to_timestamp, lag, unix_timestamp, udf, sum as spark_sum\n",
        "from pyspark.sql.types import IntegerType, DoubleType\n",
        "from pyspark.sql import Window\n",
        "from geopy.distance import geodesic"
      ],
      "metadata": {
        "id": "gRaBte5C2qvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_ais_data(file_path):\n",
        "    '''The script identifies the vessel that traveled the longest distance based on AIS data.\n",
        "    The distance calculations use the geodesic formula, ensuring accurate measurements over the Earth's surface.\n",
        "    Data is cleaned from unrealistic speeds, missing and incorrect values.'''\n",
        "\n",
        "    # Read and preprocess data\n",
        "    df = spark.read.csv(file_path, header=True, inferSchema=True).select(\n",
        "        col(\"MMSI\").cast(IntegerType()),\n",
        "        col(\"Latitude\").cast(DoubleType()),\n",
        "        col(\"Longitude\").cast(DoubleType()),\n",
        "        to_timestamp(col(\"# Timestamp\"), \"dd/MM/yyyy HH:mm:ss\").alias(\"timestamp\")\n",
        "    ).dropna(how='all', subset=[\"MMSI\", \"Latitude\", \"Longitude\", \"timestamp\"]).dropDuplicates()\n",
        "\n",
        "    # Filter out invalid latitude and longitude values\n",
        "    df = df.filter(\n",
        "        (col(\"Latitude\") >= -90) & (col(\"Latitude\") <= 90) &\n",
        "        (col(\"Longitude\") >= -180) & (col(\"Longitude\") <= 180)\n",
        "    )\n",
        "\n",
        "    # Define UDF to calculate distance using geodesic formula\n",
        "    def calculate_distance(lat1, lon1, lat2, lon2):\n",
        "        if None in (lat1, lon1, lat2, lon2):\n",
        "            return None\n",
        "        return geodesic((lat1, lon1), (lat2, lon2)).km\n",
        "\n",
        "    distance_udf = udf(calculate_distance, DoubleType())\n",
        "\n",
        "    # Create window specification to partition by MMSI and order by timestamp\n",
        "    window_spec = Window.partitionBy(\"MMSI\").orderBy(\"timestamp\")\n",
        "\n",
        "    # Calculate previous latitude, longitude, and timestamp\n",
        "    df = df.withColumn(\"prev_latitude\", lag(col(\"Latitude\")).over(window_spec)) \\\n",
        "           .withColumn(\"prev_longitude\", lag(col(\"Longitude\")).over(window_spec)) \\\n",
        "           .withColumn(\"prev_timestamp\", lag(col(\"timestamp\")).over(window_spec))\n",
        "\n",
        "    # Filter rows with complete previous coordinates and timestamps\n",
        "    df = df.filter(\n",
        "        col(\"prev_latitude\").isNotNull() & col(\"prev_longitude\").isNotNull() &\n",
        "        col(\"prev_timestamp\").isNotNull()\n",
        "    )\n",
        "\n",
        "    # Calculate distance and speed\n",
        "    df = df.withColumn(\"distance\", distance_udf(col(\"prev_latitude\"), col(\"prev_longitude\"), col(\"Latitude\"), col(\"Longitude\"))) \\\n",
        "           .withColumn(\"time_diff_hours\", (unix_timestamp(col(\"timestamp\")) - unix_timestamp(col(\"prev_timestamp\"))) / 3600) \\\n",
        "           .withColumn(\"speed_kmh\", col(\"distance\") / col(\"time_diff_hours\"))\n",
        "\n",
        "    # Filter out rows with unrealistic speed\n",
        "    max_realistic_speed_kmh = 107\n",
        "    df = df.filter(col(\"speed_kmh\") <= max_realistic_speed_kmh)\n",
        "\n",
        "    # Aggregate distances by MMSI to get the total distance traveled by each vessel and identify the vessel with the longest distance\n",
        "    longest_route_vessel = df.groupBy(\"MMSI\").agg(spark_sum(\"distance\").alias(\"total_distance\")) \\\n",
        "                              .orderBy(col(\"total_distance\").desc()).first()\n",
        "\n",
        "    if longest_route_vessel:\n",
        "        print(f\"Vessel with the longest route: MMSI {longest_route_vessel['MMSI']} with distance {longest_route_vessel['total_distance']} km\")\n",
        "    else:\n",
        "        print(\"No valid data to process.\")\n",
        "\n",
        "    spark.stop()"
      ],
      "metadata": {
        "id": "20HAng-8xSuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\"/path/to/your/aisdk-2024-05-04.csv\"\n",
        "file_path = \"/content/drive/MyDrive/Colab Notebooks/BD/Task 4/aisdk-2024-05-04.csv\"  # Update this path to your file location\n",
        "process_ais_data(file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mz3o9QOnyB9J",
        "outputId": "f40ba844-6574-4abc-b58b-5dda78b71f37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vessel with the longest route: MMSI 219133000 with distance 790.4958713586379 km\n"
          ]
        }
      ]
    }
  ]
}