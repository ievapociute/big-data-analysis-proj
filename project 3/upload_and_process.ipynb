{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pymongo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymongo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MongoClient, ASCENDING\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconcurrent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfutures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ThreadPoolExecutor, as_completed\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pymongo'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient, ASCENDING\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datetime import datetime\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading to mongo db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small comments:\n",
    "* We used local version of Mongo DB, because online one (free plan) has usage and memory restrictions\n",
    "* Chunk sizes and other parameters are appropriate for the database running local machine, computer did not crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "CSV_FILE_PATH = \"/home/jovyan/csv-files/my_data.csv\"\n",
    "CHUNK_SIZE = 10000\n",
    "MONGO_URI = \"mongodb://localhost:27017\"\n",
    "DB_NAME = 'ieva_dbs'\n",
    "COLLECTION_NAME = 'testas'\n",
    "\n",
    "# Initialize a dictionary to store unique indices for each MMSI\n",
    "mmsi_index_dict = {}\n",
    "current_index = 1\n",
    "\n",
    "# Function to read CSV in chunks\n",
    "def read_csv_in_chunks(file_path, chunk_size):\n",
    "    return pd.read_csv(file_path, chunksize=chunk_size)\n",
    "\n",
    "# Function to upload chunk to MongoDB\n",
    "def upload_chunk_to_mongodb(chunk, uri, db_name, collection_name, mmsi_index_dict):\n",
    "    global current_index\n",
    "    # Convert chunk to dictionary records\n",
    "    records = chunk.to_dict('records')\n",
    "\n",
    "    # Filter out records with null values and add unique index for each MMSI\n",
    "    filtered_records = []\n",
    "    for record in records:\n",
    "        record = {k: v for k, v in record.items() if pd.notna(v)}\n",
    "        mmsi = record.get('MMSI')\n",
    "        if mmsi:\n",
    "            if mmsi not in mmsi_index_dict:\n",
    "                mmsi_index_dict[mmsi] = current_index\n",
    "                current_index += 1\n",
    "            record['unique_index'] = mmsi_index_dict[mmsi]\n",
    "            filtered_records.append(record)\n",
    "    \n",
    "    if filtered_records:\n",
    "        # Create a new MongoClient instance for this thread\n",
    "        client = MongoClient(uri)\n",
    "        db = client[db_name]\n",
    "        collection = db[collection_name]\n",
    "        collection.insert_many(filtered_records)\n",
    "        client.close()\n",
    "\n",
    "# Main function to read CSV and upload in parallel\n",
    "def main():\n",
    "    chunks = read_csv_in_chunks(CSV_FILE_PATH, CHUNK_SIZE)\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for chunk in chunks:\n",
    "            future = executor.submit(upload_chunk_to_mongodb, chunk, MONGO_URI, DB_NAME, COLLECTION_NAME, mmsi_index_dict)\n",
    "            futures.append(future)\n",
    "        \n",
    "        for future in futures:\n",
    "            future.result()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing data from Mongo DB collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small comments:\n",
    "* We used speed as an additional filter. We used 40 knots as a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add MMSI index for better querying\n",
    "# **Updated MongoDB client connection**\n",
    "client = MongoClient(MONGO_URI)\n",
    "db = client[DB_NAME]\n",
    "collection = db[COLLECTION_NAME]\n",
    "collection.create_index([('MMSI', ASCENDING)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Updated MongoDB client connection**\n",
    "client = MongoClient(MONGO_URI)\n",
    "db = client[DB_NAME]\n",
    "source_collection = db[COLLECTION_NAME]\n",
    "target_collection = db[\"processed\"]\n",
    "\n",
    "# Helper functions\n",
    "def is_valid_coordinate(lat, lon):\n",
    "    return -90 <= lat <= 90 and -180 <= lon <= 180\n",
    "\n",
    "def parse_timestamp(timestamp):\n",
    "    try:\n",
    "        return datetime.strptime(timestamp, \"%d/%m/%Y %H:%M:%S\")\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "# This function intends to calculate vessel speed to remove obvious GPS jamming (location jumps)\n",
    "def calculate_speed(lat1, lon1, lat2, lon2, time_diff):\n",
    "    # Haversine formula to calculate the great-circle distance between two points\n",
    "    R = 3440.065  # Radius of the Earth in nautical miles\n",
    "    dlat = math.radians(lat2 - lat1)\n",
    "    dlon = math.radians(lon2 - lon1)\n",
    "    a = math.sin(dlat/2)**2 + math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) * math.sin(dlon/2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    distance = R * c  # Distance in nautical miles\n",
    "    speed = distance / (time_diff.total_seconds() / 3600)  # Speed in knots\n",
    "    return speed\n",
    "\n",
    "def process_documents(documents):\n",
    "    valid_documents = []\n",
    "    previous_doc = None\n",
    "\n",
    "    # Sort documents by parsed timestamp\n",
    "    documents.sort(key=lambda x: parse_timestamp(x[\"# Timestamp\"]))\n",
    "\n",
    "    seen_timestamps = set()\n",
    "\n",
    "    for doc in documents:\n",
    "        lat = doc.get(\"Latitude\")\n",
    "        lon = doc.get(\"Longitude\")\n",
    "        timestamp = doc.get(\"# Timestamp\")\n",
    "        \n",
    "        if not is_valid_coordinate(lat, lon):\n",
    "            continue\n",
    "        \n",
    "        date_time = parse_timestamp(timestamp)\n",
    "        if not date_time or date_time in seen_timestamps:\n",
    "            continue\n",
    "        \n",
    "        seen_timestamps.add(date_time)\n",
    "        doc[\"ParsedTimestamp\"] = date_time\n",
    "        \n",
    "        if previous_doc:\n",
    "            time_diff = date_time - previous_doc[\"ParsedTimestamp\"]\n",
    "            if time_diff.total_seconds() == 0:\n",
    "                continue\n",
    "            speed = calculate_speed(previous_doc[\"Latitude\"], previous_doc[\"Longitude\"], lat, lon, time_diff)\n",
    "            \n",
    "            if speed > 40:\n",
    "                continue\n",
    "            \n",
    "            doc[\"TimeDiff\"] = time_diff.total_seconds()\n",
    "        \n",
    "        valid_documents.append(doc)\n",
    "        previous_doc = doc\n",
    "    \n",
    "    # Remove the ParsedTimestamp key before saving\n",
    "    for doc in valid_documents:\n",
    "        doc.pop(\"ParsedTimestamp\", None)\n",
    "    \n",
    "    return valid_documents\n",
    "\n",
    "def worker(mmsi):\n",
    "    # Fetch documents for the MMSI\n",
    "    documents = list(source_collection.find({\"MMSI\": mmsi}))\n",
    "    if len(documents) < 100:\n",
    "        print(f\"Skipping MMSI {mmsi} - fewer than 100 documents.\")\n",
    "        return\n",
    "    \n",
    "    # Process and insert documents\n",
    "    processed_docs = process_documents(documents)\n",
    "    if processed_docs:\n",
    "        target_collection.insert_many(processed_docs)\n",
    "\n",
    "# Fetch unique MMSI values\n",
    "unique_mmsis = source_collection.distinct(\"MMSI\")\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = [executor.submit(worker, mmsi) for mmsi in unique_mmsis]\n",
    "    for future in as_completed(futures):\n",
    "        try:\n",
    "            future.result()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing: {e}\")\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding MMSI index for further analysis\n",
    "client = MongoClient(MONGO_URI)\n",
    "db = client[DB_NAME]\n",
    "collection = db['processed']\n",
    "collection.create_index([('MMSI', ASCENDING)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(MONGO_URI)\n",
    "db = client[DB_NAME]\n",
    "target_collection = db[\"processed\"]\n",
    "\n",
    "def extract_data(documents):\n",
    "    data = []\n",
    "    for doc in documents:\n",
    "        if \"MMSI\" in doc and \"# Timestamp\" in doc:\n",
    "            data.append({\n",
    "                \"MMSI\": doc[\"MMSI\"],\n",
    "                \"# Timestamp\": doc[\"# Timestamp\"],\n",
    "                \"TimeDiff\": doc.get(\"TimeDiff\", None)\n",
    "            })\n",
    "    return data\n",
    "\n",
    "def worker(mmsi):\n",
    "    documents = list(target_collection.find({\"MMSI\": mmsi}))\n",
    "    return extract_data(documents)\n",
    "\n",
    "unique_mmsis = target_collection.distinct(\"MMSI\")\n",
    "\n",
    "all_data = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = [executor.submit(worker, mmsi) for mmsi in unique_mmsis]\n",
    "    for future in as_completed(futures):\n",
    "        try:\n",
    "            data = future.result()\n",
    "            all_data.extend(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data for histograms\n",
    "df = pd.DataFrame(all_data)\n",
    "df = df.dropna(subset=['TimeDiff'])\n",
    "df['ParsedTimestamp'] = pd.to_datetime(df['# Timestamp'], format=\"%d/%m/%Y %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_diff_counts = df['TimeDiff'].value_counts().sort_index()\n",
    "time_diff_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_diff_counts = df['TimeDiff'].value_counts().sort_index()\n",
    "\n",
    "# Converting the time_diff_counts to a DataFrame for easier handling\n",
    "time_diff_df = time_diff_counts.reset_index()\n",
    "time_diff_df.columns = ['TimeDiff', 'Count']\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(time_diff_df['TimeDiff'], bins=3000, weights=time_diff_df['Count'], edgecolor='black')\n",
    "plt.title('Histogram of Time Differences')\n",
    "plt.xlabel('Time Difference (seconds)')\n",
    "plt.ylabel('Occurrences')\n",
    "plt.xlim(0, 500) # chosen this limit because nothing is seen besides this point\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we check for the max value when there are more than 10 occurences\n",
    "time_diff_df = time_diff_df[time_diff_df[\"Count\"] > 10]\n",
    "time_diff_df.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_counts = df['ParsedTimestamp'].value_counts().sort_index()\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(timestamp_counts.index, timestamp_counts.values, width=0.04)\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Occurrences')\n",
    "plt.title('Bar plot of Timestamps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last comments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From the histogram it can be concluded that vessel location is renewed mostly every 1-20 seconds. It is a very common practice in location tracking. However, there are quite some delta t up to 200 seconds. It's a common practice to do less frequent tracking when the ship is idling\n",
    "* From timestamp bar plot it can be seen that the occurences of signals are similar througout the day, except there is one peak at 3 pm. It can probably happen because of some error, or the tracking is set at 1 second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
